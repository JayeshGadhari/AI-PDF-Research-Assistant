# ğŸ“š AI PDFs Research Assistant using Local LLM (Ollama)

This project is an **offline, fully local document question-answering app** built with:
- ğŸ§  Open-source LLM (`llama3`) running locally via **Ollama**
- ğŸ” Free local embeddings (`sentence-transformers`)
- ğŸ—„ï¸ Vector search with **FAISS**
- ğŸ“„ PDF document processing
- ğŸ’» Interactive UI with **Streamlit**

No API keys, no cloud â€” your data stays **private and local**.

---

## âœ… Features
- Upload PDF files
- Process & chunk text
- Embed with open-source Hugging Face embeddings
- Store in FAISS vector DB
- Ask questions â€” answers are generated by `llama3` running locally with Ollama

---

## ğŸ›  Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone <https://github.com/JayeshGadhari/Information-Retrieval-System.git>
cd <Information Retrieval System>
```

---

### 2ï¸âƒ£ Create virtual environment (recommended)
```bash
python -m venv genai
# Windows
genai\Scripts\activate
# macOS/Linux
source genai/bin/activate
```

---

### 3ï¸âƒ£ Install Python dependencies
```bash
pip install -r requirements.txt
```

**Sample `requirements.txt`:**
```txt
langchain
langchain-community
faiss-cpu
sentence-transformers
PyPDF2
streamlit
python-dotenv
-e .
```

---

### 4ï¸âƒ£ Install & set up Ollama
- Download from: https://ollama.com/download
- After install, open terminal and run:
```bash
ollama pull llama3
```

To keep the local server running (optional):
```bash
ollama serve
```

---

## ğŸš€ Run the app
If you built a Streamlit UI:
```bash
streamlit run app.py
```

Or run your Python script directly:
```bash
python your_script.py
```

---

## âš¡ How it works
1. PDF files â†’ extract text
2. Text â†’ split into chunks
3. Chunks â†’ embeddings (using `sentence-transformers/all-MiniLM-L6-v2`)
4. Embeddings stored in FAISS
5. User question â†’ converted to embedding â†’ retrieve similar chunks
6. Local LLM (`llama3`) via Ollama generates answer using context

---

## ğŸ§© Stack
| Component     | What we use                                   |
|---------------|-----------------------------------------------:|
| LLM           | [llama3](https://ollama.com/library/llama3) via Ollama |
| Embeddings    | `sentence-transformers/all-MiniLM-L6-v2`     |
| Vector DB     | FAISS                                        |
| Backend       | Python + LangChain                            |
| UI            | Streamlit (if you add it)                     |

---

## ğŸ“¦ Project structure (example)
```
project/
â”œâ”€â”€ app.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ genai/                  # (your package if using -e .)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ retrieval.py
â””â”€â”€ data/                   # PDF files
```

---

## ğŸ“¸ App Preview  
![AI PDFs Research Assistant](images/SS1.png)  

---

---

## ğŸ“¸ App Preview  
![AI PDFs Research Assistant](images/SS2.png)  

---

---

## ğŸ¤ Contributing
PRs and suggestions welcome!

---

## ğŸ“„ License
MIT â€” do what you like!

---

## âœ¨ Credits
Built with:
- [LangChain](https://github.com/langchain-ai/langchain)
- [Ollama](https://ollama.com)
- [Hugging Face sentence-transformers](https://www.sbert.net/)
- [FAISS](https://github.com/facebookresearch/faiss)
